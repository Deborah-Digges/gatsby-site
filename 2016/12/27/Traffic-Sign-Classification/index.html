<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style data-href="/styles.a958a4287dc123cfa21f.css">*{box-sizing:border-box}body,html{margin:0;padding:0}html{font-family:Helvetica Neue,Helvetica,Arial,sans-serif;font-size:16px;line-height:1.5}@media (min-width:38em){html{font-size:20px}}body{color:#515151;background-color:#fff;-webkit-text-size-adjust:100%;-ms-text-size-adjust:100%}a{color:#268bd2;text-decoration:none}a strong{color:inherit}a:focus,a:hover{text-decoration:underline}h1,h2,h3,h4,h5,h6{margin-bottom:.5rem;font-weight:700;line-height:1.25;text-rendering:optimizeLegibility}h1{font-size:2rem}h2{margin-top:1rem;font-size:1.5rem}h3{margin-top:1.5rem;font-size:1.25rem}h4,h5,h6{margin-top:1rem;font-size:1rem}p{margin-top:0;margin-bottom:1rem}strong{color:#303030}dl,ol,ul{margin-top:0;margin-bottom:1rem}dt{font-weight:700}dd{margin-bottom:.5rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee;border-bottom:1px solid #fff}abbr{font-size:85%;font-weight:700;color:#555;text-transform:uppercase}abbr[title]{cursor:help;border-bottom:1px dotted #e5e5e5}code,pre{font-family:Menlo,Monaco,Courier New,monospace}code{padding:.25em .5em;font-size:85%;color:#bf616a;border-radius:3px}code,pre{background-color:#f9f9f9}pre{display:block;margin-top:0;margin-bottom:1rem;padding:1rem;font-size:.8rem;line-height:1.4;white-space:pre;white-space:pre-wrap;word-break:break-all;word-wrap:break-word}pre code{padding:0;font-size:100%;color:inherit;background-color:transparent}.highlight{margin-bottom:1rem;border-radius:4px}.highlight pre{margin-bottom:0}.gist .gist-file{font-family:Menlo,Monaco,Courier New,monospace!important}.gist .markdown-body{padding:15px}.gist pre{padding:0;background-color:transparent}.gist .gist-file .gist-data{font-size:.8rem!important;line-height:1.4}.gist code{padding:0;color:inherit;background-color:transparent;border-radius:0}blockquote{padding:.5rem 1rem;margin:.8rem 0;color:#7a7a7a;border-left:.25rem solid #e5e5e5}blockquote p:last-child{margin-bottom:0}@media (min-width:30em){blockquote{padding-right:5rem;padding-left:1.25rem}}img{display:block;max-width:100%;margin:0 0 1rem;border-radius:5px}table{margin-bottom:1rem;width:100%;border:1px solid #e5e5e5;border-collapse:collapse}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}.lead{font-size:1.25rem;font-weight:300}.message{margin-bottom:1rem;padding:1rem;color:#717171;background-color:#f9f9f9}.container{max-width:38rem;padding-left:1rem;padding-right:1rem;margin-left:auto;margin-right:auto}body,html{overflow-x:hidden}html{font-family:"PT Serif",Georgia,Times New Roman,serif}h1,h2,h3,h4,h5,h6{font-family:Calendas_Plus,Helvetica,Arial,sans-serif;font-weight:400;color:#313131;letter-spacing:-.025rem}.wrap{position:relative;width:100%}.container{max-width:28rem}@media (min-width:38em){.container{max-width:32rem}}@media (min-width:56em){.container{max-width:38rem}}.masthead{padding-top:1rem;padding-bottom:1rem;margin-bottom:3rem;border-bottom:1px solid #eee}.masthead-container{margin-right:1rem;margin-left:1rem;display:flex;justify-content:space-between;align-items:center}.masthead-nav{display:flex}.masthead-nav h4{margin-left:2em}.masthead-title{margin-top:0;margin-bottom:0;color:#505050}.masthead-title a{color:#505050}.masthead-title small{font-size:75%;font-weight:400;color:silver;letter-spacing:0}@media (max-width:48em){.masthead-title{text-align:center}}.page,.post{margin-bottom:4em}.page-title,.post-title,.post-title a{color:#303030}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-.5rem;margin-bottom:1rem;color:#9a9a9a}.related{padding-top:2rem;padding-bottom:2rem;border-top:1px solid #eee}.related-posts{padding-left:0;list-style:none}.related-posts h3{margin-top:0}.related-posts li small{font-size:75%;color:#999}.related-posts li a:hover{color:#268bd2;text-decoration:none}.related-posts li a:hover small{color:inherit}img{clear:both;margin:0 auto}.align-center{text-align:center}.disclaimer-header{background-color:#bd6666;padding:2%;margin:5%;color:#fff;border-radius:5px;-webkit-border-radius:5px;-moz-border-radius:5px}@font-face{font-family:Calendas_Plus;src:url(/static/Calendas_Plus-4988e3b187a55c809d2358cfa8e2d340.otf) format("opentype")}</style><meta name="generator" content="Gatsby 2.24.53"/><link as="script" rel="preload" href="/webpack-runtime-c4214cb327f2e6d8ac3f.js"/><link as="script" rel="preload" href="/styles-1211262dd08146354a2f.js"/><link as="script" rel="preload" href="/framework-8c8d363c63d1a9a80d21.js"/><link as="script" rel="preload" href="/app-066ba96331d18d279169.js"/><link as="script" rel="preload" href="/component---src-templates-blog-template-js-2a78e23ce79482fd97dd.js"/><link as="fetch" rel="preload" href="/page-data/2016/12/27/Traffic-Sign-Classification/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><body class="theme-base-13 sidebar-overlay"><div class="wrap"><div class="masthead"><div class="masthead-container"><h1 class="masthead-title"><a title="Home" href="/">Deb&#x27;s</a> <small>Doodles</small></h1><div class="masthead-nav"><h2 class="masthead-title"><a href="/about">About</a></h2></div></div></div><div class="container content"><div class="post"><h1 class="post-title">Traffic Sign Classification using Deep Learning</h1><span class="post-date">2016-12-27</span><div><p>The second project in the <a href="https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013">Self Driving Car Nano-degree</a> was the application of deep learning to the problem of traffic sign classification. Identifying traffic signs correctly and taking appropriate action is crucial to the operation of an autonomous vehicle.</p>
<p>The project was markedly more difficult than the previous one of lane detection for several reasons including the steep learning curve of the <a href="https://www.tensorflow.org/">Tensorflow</a> deep learning framework, the vast number of hyper-parameters that deep learning models have to tune and the extremely slow turn around time for model validation on CPUs which necessitates the setup of an environment with a powerful GPU either physically or using a cloud provider.</p>
<p>We were provided with the <a href="http://benchmark.ini.rub.de/?section=gtsrb&#x26;subsection=dataset#Downloads">German Traffic Signs</a> data set that contained about 40k training examples and 12k testing examples. The problem was one of classification which aims to assign the right class to a new image of a traffic sign by training on the provided pairs of traffic sign images and their labels. The project was broken down into:</p>
<h3>1. Exploratory Data Analysis</h3>
<p>The provided training data was examined mainly for the distribution of the various classes. The classes were found to be highly imbalanced indicating the need for data generation for the under-represented classes.</p>
<h3>2. Data Pre-processing and Augmentation</h3>
<p>The input images to the neural network went through a few pre-processing steps to help the gradient descent optimization for training the network. Pre-processing included:</p>
<p>i.  <strong>Grey Scale Conversion</strong></p>
<p>ii. <strong>Centering of images</strong> : The mean pixel value was subtracted from each pixel of the image to center the data around the origin</p>
<p>iii. <strong>Normalization</strong> : This was done by dividing each dimension by its standard deviation once it was zero-centered. This is not strictly needed for images because the relative scales of pixels are already approximately equal. This process causes each feature to have a similar range so that our gradients don't go out of control (and that we only need one global learning rate multiplier).</p>
<p>Additional data for under-represented classes was generated through a combination of the following techniques:</p>
<ul>
<li><strong>Translation</strong></li>
<li><strong>Rotation</strong></li>
<li><strong>Affine transformations</strong></li>
</ul>
<h3>3. Definition of CNN Architecture</h3>
<p><img src="../images/NN.jpg" alt="Architecture"></p>
<div class="align-center">Fig: CNN Architecture</div>
<p>The model consisted of 2 convolutional layers followed by two fully connected layers. Several methods were employed for preventing over-fitting including:</p>
<ul>
<li><strong>Max Pooling</strong></li>
<li><strong>Drop Outs</strong></li>
<li><strong>L2 Regularization</strong></li>
<li><strong>Validation Data set</strong></li>
</ul>
<h3>4. Training the model</h3>
<p>Training on a CPU became quickly unwieldy and frustrating, so I set up an environment in AWS by following the instructions <a href="http://max-likelihood.com/2016/06/18/aws-tensorflow-setup/">here</a>. Additionally, to set up jupyter on AWS and run Jupyter Notebooks remotely, I followed the instructions <a href="https://gist.github.com/iamatypeofwalrus/5183133">here</a>. The model was trained for around 100 epochs resulting in validation accuracy of 99.7% and a test accuracy of 96.2%.</p>
<h3>5. Testing on real world examples</h3>
<p>The model was tested on images from the web and the probabilities of the top 5 predicted classes were plotted. For some of the classes the model was spot on:</p>
<p><img src="../images/test-image.png" alt="Architecture"></p>
<div class="align-center">Fig: Classification of New Image</div>
<p>The model classifier mis-classified some images, but had the correct prediction within the top 5 predicted classes. For example, this 30km/h sign was mis-classified as an 80km/h sign:</p>
<p><img src="../images/wrong-classification.png" alt="Architecture"></p>
<div class="align-center">Fig: Mis-classification of 30km/h sign</div>
<p>Overall, this project was a great learning experience. However, there does still exist several areas for improvement. The project in it's current state can he found <a href="https://github.com/Deborah-Digges/SDC-ND/blob/master/P2-traffic-signs/Traffic_Signs_Recognition.ipynb">here</a>.</p></div></div></div></div></body></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/2016/12/27/Traffic-Sign-Classification/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-1841db332360a105faeb.js"],"app":["/app-066ba96331d18d279169.js"],"component---src-pages-index-js":["/component---src-pages-index-js-741ba3c07f2717a9cf69.js"],"component---src-templates-blog-template-js":["/component---src-templates-blog-template-js-2a78e23ce79482fd97dd.js"]};/*]]>*/</script><script src="/polyfill-1841db332360a105faeb.js" nomodule=""></script><script src="/component---src-templates-blog-template-js-2a78e23ce79482fd97dd.js" async=""></script><script src="/app-066ba96331d18d279169.js" async=""></script><script src="/framework-8c8d363c63d1a9a80d21.js" async=""></script><script src="/styles-1211262dd08146354a2f.js" async=""></script><script src="/webpack-runtime-c4214cb327f2e6d8ac3f.js" async=""></script></body></html>